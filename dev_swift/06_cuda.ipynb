{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_04_callbacks\")' FastaiNotebook_04_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_04_callbacks\n",
    "import TensorFlow\n",
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")\n",
    "let plt = Python.import(\"matplotlib.pyplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A CNN Mnist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = mnistDataBunch(flat: false, bs: 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let firstBatch = data.train.first(where: { _ in true })!\n",
    "let batchShape = firstBatch.xb.shape\n",
    "let batchSize = batchShape.dimensions[0]\n",
    "let exampleSideSize = batchShape.dimensions[1]\n",
    "assert(exampleSideSize == batchShape.dimensions[2])\n",
    "print(\"Batch size: \\(batchSize)\")\n",
    "print(\"Example side size: \\(exampleSideSize)\")\n",
    "\n",
    "let classCount = firstBatch.yb.shape.dimensions[1]\n",
    "print(\"Class count: \\(classCount)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CnnModel: Layer {\n",
    "    var reshapeToSquare = Reshape<Float>([-1, exampleSideSize, exampleSideSize, 1])\n",
    "    var conv1 = Conv2D<Float>(\n",
    "        filterShape: (5, 5, 1, 8),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv2 = Conv2D<Float>(\n",
    "        filterShape: (3, 3, 8, 16),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv3 = Conv2D<Float>(\n",
    "        filterShape: (3, 3, 16, 32),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    var conv4 = Conv2D<Float>(\n",
    "        filterShape: (3, 3, 32, 32),\n",
    "        strides: (2, 2),\n",
    "        padding: .same,\n",
    "        activation: relu)\n",
    "    \n",
    "    // The Python notebook uses \"AdaptiveAvgPool2d\", which I assume is different from \"AvgPool2D\".\n",
    "    // But our layers lib only has \"AvgPool2D\" and that sounds good enough for now.\n",
    "    var pool = AvgPool2D<Float>(poolSize: (2, 2), strides: (1, 1))\n",
    "    \n",
    "    var flatten = Flatten<Float>()\n",
    "    var linear = Dense<Float>(inputSize: 32, outputSize: Int(classCount))\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        // There isn't a \"sequenced\" defined with enough layers.\n",
    "        let intermediate =  input.sequenced(\n",
    "            in: context,\n",
    "            through: reshapeToSquare, conv1, conv2, conv3, conv4)\n",
    "        return intermediate.sequenced(in: context, through: pool, flatten, linear)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Test that data goes through the model as expected.\n",
    "let predictions = CnnModel().applied(to: firstBatch.xb, in: Context(learningPhase: .training))\n",
    "print(predictions.shape)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare training on CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let opt = SGD<CnnModel, Float>(learningRate: 0.4)\n",
    "func modelInit() -> CnnModel { return CnnModel() }\n",
    "\n",
    "// TODO: When TF-421 is fixed, switch back to the normal `softmaxCrossEntropy`.\n",
    "\n",
    "@differentiable(vjp: _vjpSoftmaxCrossEntropy)\n",
    "func softmaxCrossEntropy1<Scalar: TensorFlowFloatingPoint>(\n",
    "    _ features: Tensor<Scalar>, _ labels: Tensor<Scalar>\n",
    ") -> Tensor<Scalar> {\n",
    "    return Raw.softmaxCrossEntropyWithLogits(features: features, labels: labels).loss.mean()\n",
    "}\n",
    "\n",
    "@usableFromInline\n",
    "func _vjpSoftmaxCrossEntropy<Scalar: TensorFlowFloatingPoint>(\n",
    "    features: Tensor<Scalar>, labels: Tensor<Scalar>\n",
    ") -> (Tensor<Scalar>, (Tensor<Scalar>) -> (Tensor<Scalar>, Tensor<Scalar>)) {\n",
    "    let (loss, grad) = Raw.softmaxCrossEntropyWithLogits(features: features, labels: labels)\n",
    "    let batchSize = Tensor<Scalar>(features.shapeTensor[0])\n",
    "    return (loss.mean(), { v in ((v / batchSize) * grad, Tensor<Scalar>(0)) })\n",
    "}\n",
    "\n",
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy1, optimizer: opt, initializingWith: modelInit)\n",
    "learner.delegates = [Learner.TrainEvalDelegate(), Learner.AvgMetric(metrics: [accuracy])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This happens on the GPU (if you have one and it's configured correctly).\n",
    "// I tried this on a GCE 8vCPU 30GB + Tesla P100:\n",
    "// - time: ~4.3s\n",
    "// - nvidia-smi shows ~10% GPU-Util while this is running\n",
    "time {\n",
    "    try! learner.fit(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This happens on the CPU.\n",
    "// I tried this on a GCE 8vCPU 30GB + Tesla P100:\n",
    "// - time: ~6.3s\n",
    "// - nvidia-smi shows 0% GPU-Util while this is running\n",
    "time {\n",
    "    withDevice(.cpu) {\n",
    "        try! learner.fit(1)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Layer Activation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerDelegate {\n",
    "    func layer(_ name: String, didProduceActivation activation: Tensor<Float>, in context: Context) {}\n",
    "}\n",
    "\n",
    "// TODO: This should not be a global. Maybe we can put it in Context?\n",
    "var layerDelegates: [LayerDelegate] = []\n",
    "\n",
    "struct DelegatingLayer<L: Layer>: Layer\n",
    "    where L.Output == Tensor<Float>,\n",
    "          L.CotangentVector == L.AllDifferentiableVariables\n",
    "{\n",
    "    @noDerivative let name: String\n",
    "    var layer: L\n",
    "    \n",
    "    init(_ name: String, _ layer: L) {\n",
    "        self.name = name\n",
    "        self.layer = layer\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: L.Input, in context: Context) -> L.Output {\n",
    "        let activation = layer.applied(to: input, in: context)\n",
    "        \n",
    "        // Need to wrap this in a closure to prevent AD from seeing control flow.\n",
    "        ({\n",
    "            layerDelegates.forEach { $0.layer(name, didProduceActivation: activation, in: context) }\n",
    "        })()\n",
    "        \n",
    "        return activation\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CnnModelWithDelegates: Layer {\n",
    "    var reshape = Reshape<Float>([-1, exampleSideSize, exampleSideSize, 1])\n",
    "\n",
    "    var conv1 = DelegatingLayer(\n",
    "        \"conv1\",\n",
    "        Conv2D<Float>(filterShape: (5, 5, 1, 8), strides: (2, 2), padding: .same, activation: relu))\n",
    "    \n",
    "    var conv2 = DelegatingLayer(\n",
    "        \"conv2\",\n",
    "        Conv2D<Float>(filterShape: (3, 3, 8, 16), strides: (2, 2), padding: .same, activation: relu))\n",
    "        \n",
    "    var conv3 = DelegatingLayer(\n",
    "        \"conv3\",\n",
    "        Conv2D<Float>(filterShape: (3, 3, 16, 32), strides: (2, 2), padding: .same, activation: relu))\n",
    "        \n",
    "    var conv4 = DelegatingLayer(\n",
    "        \"conv4\",\n",
    "        Conv2D<Float>(filterShape: (3, 3, 32, 32), strides: (2, 2), padding: .same, activation: relu))\n",
    "    \n",
    "    // The Python notebook uses \"AdaptiveAvgPool2d\", which I assume is different from \"AvgPool2D\".\n",
    "    // But our layers lib only has \"AvgPool2D\" and that sounds good enough for now.\n",
    "    var pool = DelegatingLayer(\n",
    "        \"pool\",\n",
    "        AvgPool2D<Float>(poolSize: (2, 2), strides: (1, 1)))\n",
    "    \n",
    "    var flatten = Flatten<Float>()\n",
    "        \n",
    "    var linear = DelegatingLayer(\n",
    "        \"linear\",\n",
    "        Dense<Float>(inputSize: 32, outputSize: Int(classCount)))\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        // There isn't a \"sequenced\" defined with enough layers.\n",
    "        let intermediate =  input.sequenced(\n",
    "            in: context,\n",
    "            through: reshape, conv1, conv2, conv3, conv4)\n",
    "        return intermediate.sequenced(in: context, through: pool, flatten, linear)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationStatistics: LayerDelegate {\n",
    "    var activationMeans: [String: [Float]] = [:]\n",
    "    var activationStds: [String: [Float]] = [:]    \n",
    "    override func layer(_ name: String, didProduceActivation activation: Tensor<Float>, in context: Context) {\n",
    "        guard context.learningPhase == .training else { return }\n",
    "        activationMeans[name, default: []].append(activation.mean().scalar!)\n",
    "        activationStds[name, default: []].append(activation.standardDeviation().reshaped(to: []).scalar!)\n",
    "    }\n",
    "}\n",
    "layerDelegates.append(ActivationStatistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let opt = SGD<CnnModelWithDelegates, Float>(learningRate: 0.4)\n",
    "func modelInit() -> CnnModelWithDelegates { return CnnModelWithDelegates() }\n",
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy1, optimizer: opt, initializingWith: modelInit)\n",
    "learner.delegates = [Learner.TrainEvalDelegate(), Learner.AvgMetric(metrics: [accuracy])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This LayerDelegate stuff slows it down to ~6s/epoch.\n",
    "time {\n",
    "    try! learner.fit(2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let activationStatistics = layerDelegates[0] as! ActivationStatistics\n",
    "var names: [String] = []\n",
    "for name in activationStatistics.activationMeans.keys.sorted() {\n",
    "    plt.plot(activationStatistics.activationMeans[name])\n",
    "    names.append(name)\n",
    "}\n",
    "plt.legend(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var names: [String] = []\n",
    "for name in activationStatistics.activationStds.keys.sorted() {\n",
    "    plt.plot(activationStatistics.activationStds[name])\n",
    "    names.append(name)\n",
    "}\n",
    "plt.legend(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
