{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebooks\")' FastaiNotebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebooks\n",
    "import Python\n",
    "import Path\n",
    "import TensorFlow\n",
    "\n",
    "let np = Python.import(\"numpy\")\n",
    "let plt = Python.import(\"matplotlib.pyplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better callback cancellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath, flat: true)\n",
    "let (n,m) = (Int(xTrain.shape[0]),Int(xTrain.shape[1]))\n",
    "let c = Int((yTrain.max()+1).scalar!)\n",
    "let nHid = 50\n",
    "let data = mnistDataBunch(flat: true)\n",
    "var model = BasicModel(nIn: m, nHid: nHid, nOut: c)\n",
    "\n",
    "// Using RiemannSGD (fancy name for generic without momentum) because the normal one doesn't let me change the LR.\n",
    "let opt = RiemannSGD<BasicModel, Float>(learningRate: 1e-2)\n",
    "func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}\n",
    "\n",
    "// TODO: When TF-421 is fixed, switch back to the normal `softmaxCrossEntropy`.\n",
    "@differentiable(vjp: _vjpSoftmaxCrossEntropy)\n",
    "func softmaxCrossEntropy1<Scalar: TensorFlowFloatingPoint>(\n",
    "    _ features: Tensor<Scalar>, _ labels: Tensor<Scalar>\n",
    ") -> Tensor<Scalar> {\n",
    "    return Raw.softmaxCrossEntropyWithLogits(features: features, labels: labels).loss.mean()\n",
    "}\n",
    "\n",
    "@usableFromInline\n",
    "func _vjpSoftmaxCrossEntropy<Scalar: TensorFlowFloatingPoint>(\n",
    "    features: Tensor<Scalar>, labels: Tensor<Scalar>\n",
    ") -> (Tensor<Scalar>, (Tensor<Scalar>) -> (Tensor<Scalar>, Tensor<Scalar>)) {\n",
    "    let (loss, grad) = Raw.softmaxCrossEntropyWithLogits(features: features, labels: labels)\n",
    "    let batchSize = Tensor<Scalar>(features.shapeTensor[0])\n",
    "    return (loss.mean(), { v in ((v / batchSize) * grad, Tensor<Scalar>(0)) })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy1, optimizer: opt, initializingWith: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Extension of learner because it must take learner as a template paramter.\n",
    "extension Learner {\n",
    "    public class TestCallback : Delegate {\n",
    "        // Alas, this is \"after_batch\" because \"after_step\" did not exist.\n",
    "        public override func batchDidFinish(learner: Learner) throws {\n",
    "            print(learner.currentIter)\n",
    "            if learner.currentIter >= 10 {\n",
    "                throw LearnerAction.stop\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.delegates = [Learner.TrainEvalDelegate(), Learner.TestCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func plot<S1, S2>(_ arr1: [S1], _ arr2: [S2], logScale:Bool = false, xLabel: String=\"\", yLabel: String = \"\") \n",
    "    where S1:PythonConvertible, S2:PythonConvertible{\n",
    "    plt.figure(figsize: [6,4])\n",
    "    let (npArr1, npArr2) = (np.array(arr1), np.array(arr2))\n",
    "    if logScale {plt.xscale(\"log\")} \n",
    "    if !xLabel.isEmpty {plt.xlabel(xLabel)}\n",
    "    if !yLabel.isEmpty {plt.ylabel(yLabel)}    \n",
    "    let fig = plt.plot(npArr1, npArr2)\n",
    "    plt.show(fig)\n",
    "}\n",
    "\n",
    "// export\n",
    "extension Learner where O.Scalar: PythonConvertible{\n",
    "    public func makeRecorder() -> Recorder {\n",
    "        return Recorder()\n",
    "    }\n",
    "\n",
    "    public class Recorder: Delegate {\n",
    "        public var losses: [Loss] = []\n",
    "        public var lrs: [O.Scalar] = []\n",
    "        \n",
    "        public override func batchDidFinish(learner: Learner) {\n",
    "            if learner.inTrain {\n",
    "                losses.append(learner.currentLoss)\n",
    "                lrs.append(learner.optimizer.learningRate)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        public func plotLosses(){\n",
    "            plot(Array(0..<losses.count), losses.map{$0.scalar}, xLabel:\"iteration\", yLabel:\"loss\")\n",
    "        }\n",
    "        \n",
    "        public func plotLRs(){\n",
    "            plot(Array(0..<lrs.count), lrs, xLabel:\"iteration\", yLabel:\"lr\")\n",
    "        }\n",
    "        \n",
    "        public func plotLRFinder(){\n",
    "            plot(lrs, losses.map{$0.scalar}, logScale: true, xLabel:\"lr\", yLabel:\"loss\")\n",
    "        }\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "extension Learner where O.Scalar: PythonConvertible{\n",
    "    public var recorder: Recorder? {\n",
    "        for callback in delegates {\n",
    "            if let recorder = callback as? Recorder { return recorder }\n",
    "        }\n",
    "        return nil\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Hack\n",
    "extension Learner {\n",
    "    func makeTrainEvalDelegate() -> TrainEvalDelegate { return TrainEvalDelegate() }\n",
    "}\n",
    "\n",
    "/// A non-generalized learning rate scheduler\n",
    "extension Learner where O.Scalar == Float {\n",
    "\n",
    "    public class ParamScheduler: Delegate {\n",
    "        public typealias ScheduleFunc = (Float) -> Float\n",
    "\n",
    "        // A learning rate schedule from step to float.\n",
    "        public var scheduler: ScheduleFunc\n",
    "        \n",
    "        public init(scheduler: @escaping (Float) -> Float) {\n",
    "            self.scheduler = scheduler\n",
    "        }\n",
    "        \n",
    "        override public func batchWillStart(learner: Learner) {\n",
    "            learner.optimizer.learningRate = scheduler(learner.pctEpochs/Float(learner.epochCount))\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: You may want to also add something that saves the model before running this, and loads it back after running - otherwise you'll lose your weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func expSchedule(start: Float, end: Float, pct: Float) -> Float {\n",
    "    return start * pow(end / start, pct)\n",
    "}\n",
    "\n",
    "func makeAnnealer(start: Float, end: Float, schedule: @escaping (Float, Float, Float) -> Float) -> (Float) -> Float { \n",
    "    return { pct in return schedule(start, end, pct) }\n",
    "}\n",
    "\n",
    "/// LR Finder test\n",
    "extension Learner where O.Scalar == O.Scalar: BinaryFloatingPoint {\n",
    "\n",
    "    public class LRFinder2: Delegate {\n",
    "        public typealias ScheduleFunc = (Float) -> Float\n",
    "\n",
    "        // A learning rate schedule from step to float.\n",
    "        public var scheduler: ScheduleFunc\n",
    "        public var numIter: Int\n",
    "        public var minLoss: Float? = nil\n",
    "        \n",
    "        public init(start: Float = 1e-5, end: Float = 10, numIter: Int = 100) {\n",
    "            scheduler = makeAnnealer(start: start, end: end, schedule: expSchedule)\n",
    "            self.numIter = numIter\n",
    "        }\n",
    "        \n",
    "        override public func batchWillStart(learner: Learner) {\n",
    "            learner.optimizer.learningRate = O.Scalar(scheduler(Float(learner.currentIter)/Float(numIter)))\n",
    "        }\n",
    "        \n",
    "        override public func batchDidFinish(learner: Learner) throws {\n",
    "            if minLoss == nil {minLoss = learner.currentLoss.scalar}\n",
    "            else { \n",
    "                if learner.currentLoss.scalarized() < minLoss! { minLoss = learner.currentLoss.scalarized()}\n",
    "                if learner.currentLoss.scalarized() > 4 * minLoss! { throw LearnerAction.stop }\n",
    "                if learner.currentIter >= numIter { throw LearnerAction.stop }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    func makeLRFinder2(start: Float = 1e-5, end: Float = 10, numIter: Int = 100)-> LRFinder2 {\n",
    "        return LRFinder2(start: start, end: end, numIter: numIter)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: In fastai we also use exponential smoothing on the loss. For that reason we check for `best_loss*3` instead of `best_loss*10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeRecorder(), learner.makeLRFinder2()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plotLRFinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookToScript(fname: (Path.cwd / \"05b_early_stopping.ipynb\").string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
